!pip install transformers ipywidgets pandas -q

import torch
import pandas as pd
import os
import zipfile
from IPython.display import display, HTML
import ipywidgets as widgets
from google.colab import files
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Apply custom CSS for dark theme UI
display(HTML("""
<style>
    .chat-container {
        background-color: #2b2b2b;
        padding: 20px;
        border-radius: 10px;
        max-width: 800px;
        margin: 0 auto;
        font-family: 'Arial', sans-serif;
    }
    .chat-title {
        color: #64b5f6;
        text-align: center;
        margin-bottom: 20px;
    }
    .chat-description {
        color: #b0bec5;
        text-align: center;
        margin-bottom: 20px;
    }
    .chat-input {
        background-color: #424242;
        color: #ffffff;
        border: 1px solid #616161;
        border-radius: 5px;
        padding: 10px;
        margin-bottom: 10px;
    }
    .chat-button {
        background-color: #1976d2 !important;
        color: #ffffff !important;
        border-radius: 5px !important;
        padding: 10px 20px !important;
        margin: 5px !important;
    }
    .chat-button:hover {
        background-color: #1565c0 !important;
    }
    .chat-output {
        background-color: #424242;
        color: #ffffff;
        border: 1px solid #616161;
        border-radius: 5px;
        padding: 15px;
        max-height: 400px;
        overflow-y: auto;
        margin-top: 10px;
    }
    .user-message {
        background-color: #4fc3f7;
        color: #000000;
        padding: 10px;
        margin: 5px 0;
        border-radius: 10px;
        display: inline-block;
        max-width: 70%;
        word-wrap: break-word;
    }
    .ai-message {
        background-color: #757575;
        color: #ffffff;
        padding: 10px;
        margin: 5px 0;
        border-radius: 10px;
        display: inline-block;
        max-width: 70%;
        word-wrap: break-word;
    }
</style>
"""))

# Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Function to prepare Poetry Foundation dataset from uploaded ZIP
def prepare_poetry_dataset():
    """
    Process uploaded Poetry Foundation Poems ZIP dataset
    Returns:
        file_path: Path to the training text file
    """
    print("Please upload the dataset ZIP file (dataset of poetry.zip)")
    uploaded = files.upload()
    zip_file = list(uploaded.keys())[0]

    with zipfile.ZipFile(zip_file, 'r') as zip_ref:
        zip_ref.extractall("poetry_dataset")

    csv_file = os.path.join("poetry_dataset", "PoetryFoundationData.csv")
    if not os.path.exists(csv_file):
        raise FileNotFoundError(f"Expected CSV file not found in poetry_dataset")

    df = pd.read_csv(csv_file)
    poems = df['Poem'].dropna().tolist()

    file_path = "poetry_train.txt"
    with open(file_path, "w", encoding="utf-8") as f:
        f.write("\n".join(poems))
    return file_path

# Function to load pre-trained model and tokenizer
def load_model(model_name="gpt2"):
    """
    Load a pre-trained model and tokenizer
    Args:
        model_name: The name of the model to load (default: "gpt2")
    Returns:
        model: The loaded model
        tokenizer: The loaded tokenizer
    """
    try:
        tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        tokenizer.pad_token = tokenizer.eos_token
        model = GPT2LMHeadModel.from_pretrained(model_name).to(device)
        return model, tokenizer
    except Exception as e:
        print(f"Error loading model: {str(e)}")
        raise

# Function to generate text
def generate_text(model, tokenizer, prompt, max_length=150, temperature=0.7):
    """
    Generate text based on a prompt
    Args:
        model: The model to use for generation
        tokenizer: The tokenizer to use for generation
        prompt: The prompt to generate text from
        max_length: The maximum length of the generated text
        temperature: The temperature to use for generation
    Returns:
        generated_text: The generated text
    """
    try:
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512, padding=True).to(device)
        input_ids = inputs["input_ids"]
        attention_mask = inputs["attention_mask"]

        output = model.generate(
            input_ids,
            attention_mask=attention_mask,
            max_length=max_length,
            temperature=temperature,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            top_p=0.95,
            top_k=50,
            num_return_sequences=1,
            no_repeat_ngram_size=2
        )

        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
        return generated_text
    except Exception as e:
        print(f"Error generating text: {str(e)}")
        return "Sorry, I couldn't generate a response. Please try again."

# Function to create a chat interface
def chat_ui(model, tokenizer):
    """
    Create an enhanced chat-like interface for interacting with the model
    Args:
        model: The model
        tokenizer: The tokenizer used with the model
    """
    conversation_history = []

    prompt_input = widgets.Textarea(
        value='',
        placeholder='Type your question or prompt here...',
        description='You:',
        layout=widgets.Layout(width='100%', height='100px'),
        style={'description_width': 'initial', 'description_color': '#ffffff'}
    )

    send_button = widgets.Button(
        description='Send',
        tooltip='Click to send your prompt and get a response',
        layout=widgets.Layout(width='auto')
    )

    clear_button = widgets.Button(
        description='Clear Chat',
        tooltip='Click to clear the conversation history',
        layout=widgets.Layout(width='auto')
    )

    output_area = widgets.HTML(
        value='',
        layout=widgets.Layout(width='100%', min_height='200px', max_height='400px', overflow_y='auto')
    )

    def update_conversation_display():
        html_content = ""
        for user_msg, ai_msg in conversation_history:
            html_content += f"""
            <div style='text-align: right; margin: 5px 0;'>
                <span class='user-message'>You: {user_msg}</span>
            </div>
            <div style='text-align: left; margin: 5px 0;'>
                <span class='ai-message'>AI: {ai_msg}</span>
            </div>
            """
        output_area.value = html_content

    def on_send_button_clicked(b):
        if not prompt_input.value.strip():
            output_area.value = "<p style='color: #ef5350;'>Please enter a prompt!</p>"
            return
        user_prompt = prompt_input.value.strip()
        conversation_history.append((user_prompt, "Generating response..."))
        update_conversation_display()

        response = generate_text(model, tokenizer, user_prompt)
        conversation_history[-1] = (user_prompt, response)
        update_conversation_display()
        prompt_input.value = ""

    def on_clear_button_clicked(b):
        conversation_history.clear()
        output_area.value = ""

    send_button.on_click(on_send_button_clicked)
    clear_button.on_click(on_clear_button_clicked)

    ui = widgets.VBox([
        widgets.HTML("<h2 class='chat-title'>Chat with AI (GPT-2)</h2>"),
        widgets.HTML("<p class='chat-description'>Ask questions or provide prompts, and the AI will respond using the GPT-2 model.</p>"),
        prompt_input,
        widgets.HBox([send_button, clear_button], layout=widgets.Layout(justify_content='flex-start')),
        output_area
    ], layout=widgets.Layout(padding='20px'))

    display(widgets.HTML("<div class='chat-container'>"))
    display(ui)
    display(widgets.HTML("</div>"))

# Main function to run the chatbot
def run_chatbot():
    """
    Prepare dataset, load model, and launch chat interface
    """
    print("Preparing Poetry Foundation dataset...")
    train_file = prepare_poetry_dataset()

    print("Loading GPT-2 model...")
    model, tokenizer = load_model("gpt2")

    print("AI model trained")
    print("GPT-2 model trained")
    print("Chat with Our Text Generator")
    chat_ui(model, tokenizer)

# Run the chatbot
run_chatbot()
